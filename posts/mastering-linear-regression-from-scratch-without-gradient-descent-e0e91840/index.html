<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="theme-color" content="#FFFFFF">
    <title>Mastering Linear Regression: From Scratch without Gradient Descent</title>
    <meta name="robots" content="index,follow">
    <meta name="title" content="Mastering Linear Regression: From Scratch without Gradient Descent">
    <meta name="description" content="Learn how to implement linear regression from scratch without gradient descent, with a focus on the...">
      
    <!-- Twitter meta tags -->
    <meta name="twitter:description" content="Learn how to implement linear regression from scratch without gradient descent, with a focus on the...">
    <meta name="twitter:title" content="Mastering Linear Regression: From Scratch without Gradient Descent">
    <meta property="twitter:domain" content="blog.thefcraft.site">
    <meta property="twitter:url" content="https://blog.thefcraft.site/posts/mastering-linear-regression-from-scratch-without-gradient-descent-e0e91840">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Add other Twitter meta tags as needed -->
      
    <!-- Open Graph meta tags -->
    <meta property="og:description" content="Learn how to implement linear regression from scratch without gradient descent, with a focus on the...">
    <meta property="og:title" content="Mastering Linear Regression: From Scratch without Gradient Descent">
    <meta property="og:site_name" content="Blogifyr">
    <meta property="og:url" content="https://blog.thefcraft.site/posts/mastering-linear-regression-from-scratch-without-gradient-descent-e0e91840">
    <!-- Add other Open Graph meta tags as needed -->
      
 

    <!-- Favicon -->
    <link rel="icon" type="image/x-icon" href="/static/img/icon.webp">
    
    <!-- CSS and JavaScript links -->
    <link rel="stylesheet" href="/static/css/style.css">
    <link rel="stylesheet" href="/static/css/nav.css">
    <link rel="stylesheet" href="/static/css/contentStyle.css">

    <script type="text/javascript" src="/static/js/index.js"></script>

    <script>
        // Function to add dark mode stylesheet
        function addDarkMode() {
            let head = document.getElementsByTagName('HEAD')[0];
            let link = document.createElement('link');
            link.rel = 'stylesheet';
            link.type = 'text/css';
            link.href = '/static/css/darkmode.css';
            link.id = 'dark-mode-stylesheet';  // Adding an id for easy removal
            head.appendChild(link);
        }

        // Function to remove dark mode stylesheet
        function removeDarkMode() {
            let darkModeStylesheet = document.getElementById('dark-mode-stylesheet');
            if (darkModeStylesheet) {
                darkModeStylesheet.parentNode.removeChild(darkModeStylesheet);
            }
        }

        function isDarkMode() {
            let darkModeStylesheet = document.getElementById('dark-mode-stylesheet');
            if (darkModeStylesheet) {
                return true;
            }
            return false;
        }

        function changeTheme(elem) {
            if(document.getElementById('dark-mode-stylesheet')){
                removeDarkMode();
            }else{
                addDarkMode();
            }   
        }

        </script>



    

<!-- codeHighlighter -->
<!-- <link rel="stylesheet" href="https://unpkg.com/highlightjs@9.16.2/styles/atom-one-dark.css"> -->
<link rel="stylesheet" href="https://unpkg.com/highlightjs@9.16.2/styles/atom-one-light.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>
  // hljs.initHighlightingOnLoad();
  hljs.highlightAll()
</script>

<!-- CDN link to include MathJax: fro LaTeX  -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>


<link rel="icon" type="image/x-icon" href="/static/img/icon.webp">
<link rel="stylesheet" href="/static/css/blogNewUser.css">
<!-- add SEO -->
<meta property="og:type" content="article">
<meta name="author" content="ThefCraft">
<meta name="keywords" content="keywords in head of the post">
<meta property="article:published_time" content="2024-06-17T19:17:32.390670Z">
<meta name="twitter:creator" content="@ThefCraft">
<meta name="twitter:image" content="https://blog.thefcraft.site/static/img/posts/linearRegression/linearregression.png">
<meta property="og:image" content="https://blog.thefcraft.site/static/img/posts/linearRegression/linearregression.png">


</head>
<body>
    
<!-- if login... -->
<nav class="nav-transparent">
    <div class="nav-left-container">
        <a href="/" class="dnon">
            <svg viewBox="0 0 410 109" style="height: 100%;" preserveAspectRatio="xMidYMid meet" class="css-8aabad"
                id="dgegagha">
                <defs id="SvgjsDefs2161">
                    <linearGradient id="SvgjsLinearGradient2168">
                        <stop id="SvgjsStop2169" stop-color="#006838" offset="0"></stop>
                        <stop id="SvgjsStop2170" stop-color="#96cf24" offset="1"></stop>
                    </linearGradient>
                </defs>
                <g id="SvgjsG2162" featurekey="symbolContainer" transform="matrix(1,0,0,1,0,0)" fill="#111">
                    <rect width="110" height="109"></rect>
                </g>
                <g id="SvgjsG2163" featurekey="monogramFeature-0"
                    transform="matrix(1.8430753250050795,0,0,1.8430753250050795,15.466031993639358,-17.069509952010947)"
                    fill="#fff">
                    <path
                        d="M29.28 37.08 c6.9 0.96 11.16 5.16 11.16 10.92 c0 7.08 -6.24 12 -20.1 12 l-16.44 0 c-0.84 0 -1.44 -0.42 -1.44 -1.14 l0 -0.24 c0 -1.68 2.4 -2.4 2.4 -5.16 l0 -29.52 c0 -2.76 -2.4 -3.48 -2.4 -5.16 l0 -0.24 c0 -0.72 0.6 -1.14 1.44 -1.14 l16.62 0 c10.8 0 15.72 4.8 15.72 10.8 c0 4.5 -2.76 7.8 -6.96 8.88 z M15.66 24.72 l0 7.56 c0 1.98 1.32 3.12 3.24 3.12 l3 0 c3.06 0 5.04 -2.22 5.04 -5.64 c0 -5.16 -2.7 -8.16 -6.78 -8.16 l-1.68 0 c-1.56 0 -2.82 0.72 -2.82 3.12 z M23.04 55.8 c3.84 0 6 -2.52 6 -6.24 c0 -4.44 -3.66 -9.96 -10.86 -9.96 l-2.52 0 l0 11.1 c0 2.88 1.74 5.1 4.5 5.1 l2.88 0 z">
                    </path>
                </g>
                <g id="SvgjsG2164" featurekey="nameFeature-0"
                    transform="matrix(2.0408161449540345,0,0,2.0408161449540345,125.1020420306199,-6.102038138072011)"
                    fill="currentColor">
                    <path
                        d="M15.88 24.6 c3.68 0.84 6.08 3.88 6.08 7.4 c0 4.92 -3.2 8 -10.12 8 l-9.44 0 l0 -28 l9.56 0 c5.48 0 8.2 3.12 8.2 6.76 c0 2.68 -1.52 4.84 -4.28 5.84 z M11.72 14.96 l-6 0 l0 8.56 l6 0 c3.2 0 5.16 -1.76 5.16 -4.32 c0 -2.64 -1.84 -4.24 -5.16 -4.24 z M11.72 37.04 c4.8 0 6.92 -1.96 6.92 -5.32 c0 -2.92 -2.12 -5.44 -6.72 -5.44 l-6.2 0 l0 10.76 l6 0 z M28.919999999999998 11.719999999999999 l0 28.28 l-3.28 0 l0 -28.28 l3.28 0 z M43.239999999999995 18.84 c4.72 0 10.68 3.6 10.68 10.8 s-5.96 10.76 -10.68 10.76 s-10.64 -3.56 -10.64 -10.76 s5.92 -10.8 10.64 -10.8 z M43.239999999999995 21.76 c-3.36 0 -7.44 2.6 -7.44 7.88 c0 5.24 4.08 7.84 7.44 7.84 c3.4 0 7.48 -2.6 7.48 -7.84 c0 -5.28 -4.08 -7.88 -7.48 -7.88 z M74.64 19.2 l3.28 0 l0 20.8 c0 6.28 -5.52 8.24 -9.52 8.24 c-3.2 0 -5.24 -0.84 -6.24 -1.56 l0 -2.8 c1 0.72 3.04 1.56 6.24 1.56 c3.6 0 6.24 -1.96 6.24 -5.44 l0 -4 c-0.96 2.4 -3.8 4.4 -7.48 4.4 c-4.72 0 -10.44 -3.56 -10.44 -10.8 c0 -7.16 5.72 -10.76 10.44 -10.76 c3.68 0 6.52 2.04 7.48 4.28 l0 -3.92 z M67.52 37.48 c3.56 0 7.28 -2.52 7.28 -7.88 c0 -5.32 -3.72 -7.84 -7.28 -7.84 c-3.52 0 -7.6 2.6 -7.6 7.84 c0 5.28 4.08 7.88 7.6 7.88 z M85.76 11.719999999999999 l0 2.92 l-3.28 0 l0 -2.92 l3.28 0 z M85.76 19.2 l0 20.8 l-3.28 0 l0 -20.8 l3.28 0 z M97.68 14.079999999999998 c-2.2 0 -3.28 1.08 -3.28 3.64 l0 1.48 l5.88 0 l0 2.76 l-5.88 0 l0 18.04 l-3.28 0 l0 -18.04 l-2.28 0 l0 -2.76 l2.28 0 l0 -1.48 c0 -5.48 3.64 -6.4 6.28 -6.4 c1.72 0 3.32 0.48 4.52 1.24 l-1.2 2.28 c-0.92 -0.44 -1.84 -0.76 -3.04 -0.76 z M121.04 19.16 l3.28 0 l-12.64 28.44 l-3.28 0 l3.72 -8.28 l-9 -20.16 l3.28 0 l7.32 16.56 z M138.6 18.88 c0.36 0 0.68 0 1 0.08 l0 3.08 c-0.32 -0.08 -0.68 -0.08 -0.96 -0.08 c-3.96 0 -7.2 2.84 -7.76 7.52 l0 10.52 l-3.28 0 l0 -20.8 l3.28 0 l0 5.48 c0.76 -3.08 3.6 -5.8 7.72 -5.8 z">
                    </path>
                </g>
            </svg>
        </a>
        <form action="/search/index.html" method="get">
            <div class="search-field" >
                <div class="search-field-img">
                    <button type="submit" style="all: unset;">
                        <svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.1 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0zm6.94-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .8-.79l-3.74-3.73A8.05 8.05 0 0 0 11.04 3v.01z" fill="currentColor"></path></svg>
                    </button>
                </div>
                <div>
                    <input value="" type="text" name="query" id="query" placeholder="Search">
                </div>
            </div>
        </form>
    </div>
    <div class="nav-right-container">
        <a href="/">Home</a>
        <div>
            <input type="checkbox" class="checkbox" id="checkbox">
            <label for="checkbox" class="checkbox-label">
                <svg class="fa-moon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--!Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d="M223.5 32C100 32 0 132.3 0 256S100 480 223.5 480c60.6 0 115.5-24.2 155.8-63.4c5-4.9 6.3-12.5 3.1-18.7s-10.1-9.7-17-8.5c-9.8 1.7-19.8 2.6-30.1 2.6c-96.9 0-175.5-78.8-175.5-176c0-65.8 36-123.1 89.3-153.3c6.1-3.5 9.2-10.5 7.7-17.3s-7.3-11.9-14.3-12.5c-6.3-.5-12.6-.8-19-.8z"/></svg>
                <svg class="fa-sun" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--!Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d="M375.7 19.7c-1.5-8-6.9-14.7-14.4-17.8s-16.1-2.2-22.8 2.4L256 61.1 173.5 4.2c-6.7-4.6-15.3-5.5-22.8-2.4s-12.9 9.8-14.4 17.8l-18.1 98.5L19.7 136.3c-8 1.5-14.7 6.9-17.8 14.4s-2.2 16.1 2.4 22.8L61.1 256 4.2 338.5c-4.6 6.7-5.5 15.3-2.4 22.8s9.8 13 17.8 14.4l98.5 18.1 18.1 98.5c1.5 8 6.9 14.7 14.4 17.8s16.1 2.2 22.8-2.4L256 450.9l82.5 56.9c6.7 4.6 15.3 5.5 22.8 2.4s12.9-9.8 14.4-17.8l18.1-98.5 98.5-18.1c8-1.5 14.7-6.9 17.8-14.4s2.2-16.1-2.4-22.8L450.9 256l56.9-82.5c4.6-6.7 5.5-15.3 2.4-22.8s-9.8-12.9-17.8-14.4l-98.5-18.1L375.7 19.7zM269.6 110l65.6-45.2 14.4 78.3c1.8 9.8 9.5 17.5 19.3 19.3l78.3 14.4L402 242.4c-5.7 8.2-5.7 19 0 27.2l45.2 65.6-78.3 14.4c-9.8 1.8-17.5 9.5-19.3 19.3l-14.4 78.3L269.6 402c-8.2-5.7-19-5.7-27.2 0l-65.6 45.2-14.4-78.3c-1.8-9.8-9.5-17.5-19.3-19.3L64.8 335.2 110 269.6c5.7-8.2 5.7-19 0-27.2L64.8 176.8l78.3-14.4c9.8-1.8 17.5-9.5 19.3-19.3l14.4-78.3L242.4 110c8.2 5.7 19 5.7 27.2 0zM256 368a112 112 0 1 0 0-224 112 112 0 1 0 0 224zM192 256a64 64 0 1 1 128 0 64 64 0 1 1 -128 0z"/></svg>
                <span class="ball"></span>
            </label>
        </div>
        <div class="btn dnon" onclick="window.location.href = '#newsletter'">Get started</div>
    </div>
</nav>

<div class="root">
    <div class="container">

        <div class="heading">
            <h1>Mastering Linear Regression: From Scratch without Gradient Descent</h1><h2 style="font-weight:300;" id="blog-desc">Learn how to implement linear regression from scratch without gradient descent, with a focus on the...</h2><div class="heading-userData-container">
                <img alt="ThefCraft" src="/static/userPNG/c5f67cbc-b58f-46cc-864a-5e48b2a6d582.jpg" width="44" height="44" loading="lazy">
                <div class="heading-userData">
                    <div>
                        <a href="/user/ThefCraft" id="userName">ThefCraft</a> ¬∑ <a href="#newsletter" class="follow">Follow</a>
                    </div>

                    <div style="font-size: 14px;" id="readTime">
                        13 min ¬∑ Jun 17, 2024
                    </div>
                </div>
            </div><div class="blog-reaction-div">
                <div class="blog-reaction">
                    <div>
                        <svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" fill="currentColor" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z"></path></svg>
                    </div>
                    <div>141</div>
                    <div style="margin-left: 16px;">
                        <svg width="24" height="24" viewBox="0 0 24 24" class="jp"><path fill="currentColor" d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg>
                    </div>
                    <div>849</div>
                </div>
                <div class="blog-save-share">
                    <div id="speak-nav">
                        <svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0zm9-10a10 10 0 1 0 0 20 10 10 0 0 0 0-20zm3.38 10.42l-4.6 3.06a.5.5 0 0 1-.78-.41V8.93c0-.4.45-.63.78-.41l4.6 3.06c.3.2.3.64 0 .84z" fill="currentColor"></path></svg>
                    </div>
                    <div id="share-nav">
                        <svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z" fill="currentColor"></path></svg>
                    </div>
                </div>
            </div>
        </div>


        

        <div class="content" id="content">
            <h1>Mastering Linear Regression: From Scratch without Gradient Descent.</h1>

<p><img src="https://blog.thefcraft.site/static/img/posts/linearRegression/linearregression.png" alt="linear regression" /> <figcaption>linear regression</figcaption></p>

<h2>Introduction</h2>

<p>Linear Regression is a method in which we tries to best-fit a line on the dataset in a way that that minimizes the mean square error. In other words Linear Regression is a method used to define a relationship between a dependent variable (Y) and independent variables (x1, x2, ..., xk). Which is simply written as :</p>

<p>
\[ \hat{y} = {m_1}\times{x_1} + {m_2}\times{x_2} + \ldots + {m_k}\times{x_k} + b \]
or simply
\[ \hat{y} = {M}{X} + b \]
</p>

<p>where:</p>

<ul>
<li>≈∑ is the predicted value (dependent variable).</li>
<li>X is the independent variables.</li>
<li>M is the slope of the line (weights).</li>
<li>b is the y-intercept (bias).</li>
</ul>

<h3>Example Scenario</h3>

<p>Imagine we wants to predict the house price based on some parameters like rooms and area etc. Then we could establish a relationship between its price and the input parameters using linear regression algorithm.</p>

<div class="separator"><span>‚Ä¢ ‚Ä¢ ‚Ä¢</span></div>

<h2>Section 2: Mathematical Foundations</h2>

<h3>Mean Squared Error (MSE)</h3>

<p>To measure the error of our linear regression model, we use the Mean Squared Error (MSE), which calculates the mean of the squared difference between the actual and predicted values:</p>

<p>
\[ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2 \]
</p>

<p>where:</p>

<ul>
<li>n is the number of data points.</li>
<li>yùíä is the actual value.</li>
<li>≈∑ùíä is the predicted value.</li>
</ul>

<p>On Now you know what mean squared error is but we want to minimize this error to accurately fit the line on the data points. but how to minimize this error?</p>

<h3>Finding Minima using Derivatives</h3>

<p><img src="https://www.a-levelmathstutor.com/images/calculus/maxmin_1.jpg" alt="https://www.a-levelmathstutor.com/images/calculus/maxmin<em>1.jpg" /> <figcaption>https://www.a-levelmathstutor.com/images/calculus/maxmin</em>1.jpg</figcaption></p>

<p>In a smoothly changing function a minimum is always where the function flattens out (except for a <a href="https://en.wikipedia.org/wiki/Saddle_point">saddle point</a>).</p>

<h3>Partial Derivatives</h3>

<p>The partial derivatives of the MSE with respect to m:</p>

<p>
\[ \frac{\partial \text{MSE}}{\partial m} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2 \]
\[ \frac{\partial \text{MSE}}{\partial m} = \frac{1}{n} \sum_{i=1}^{n} (y_i - (m {x_i} + b))^2 \]
\[ \frac{\partial \text{MSE}}{\partial m} = \frac{1}{n} \sum_{i=1}^{n} (y_i - (m {x_i} + b))(-2{x_i}) \]
\[ \frac{\partial \text{MSE}}{\partial m} = -\frac{2}{n} \sum_{i=1}^{n} x_i (y_i - (m {x_i} + b)) \]
\[ \frac{\partial \text{MSE}}{\partial m} = -\frac{2}{n} \sum_{i=1}^{n} x_i (y_i - \hat{y_i}) \]
</p>

<p>The partial derivatives of the MSE with respect to b:</p>

<p>
\[ \frac{\partial \text{MSE}}{\partial b} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2 \]
\[ \frac{\partial \text{MSE}}{\partial b} = \frac{1}{n} \sum_{i=1}^{n} (y_i - (m {x_i} + b))^2 \]
\[ \frac{\partial \text{MSE}}{\partial b} = \frac{1}{n} \sum_{i=1}^{n} (y_i - (m {x_i} + b))(-2) \]
\[ \frac{\partial \text{MSE}}{\partial b} = -\frac{2}{n} \sum_{i=1}^{n} (y_i - (m {x_i} + b)) \]
\[ \frac{\partial \text{MSE}}{\partial b} = -\frac{2}{n} \sum_{i=1}^{n} (y_i - \hat{y_i}) \]
</p>

<h3>Equating the gradients of the error function to zero</h3>

<p>Equating the partial derivatives of the MSE with respect to b to zero:</p>

<p>
\[ \frac{\partial \text{MSE}}{\partial b} = -\frac{2}{n} \sum_{i=1}^{n} (y_i - (m {x_i} + b)) = 0 \]
\[ -\frac{2}{n} \sum_{i=1}^{n} (y_i - m {x_i} - b) = 0 \]
\[ -\frac{2}{n} \sum_{i=1}^{n} (y_i) +\frac{2}{n} \sum_{i=1}^{n} ( m {x_i} )  +\frac{2}{n} \sum_{i=1}^{n} (b)  = 0 \]
\[ -\frac{1}{n} \sum_{i=1}^{n} (y_i) +\frac{1}{n} \sum_{i=1}^{n} ( m {x_i} )  +\frac{1}{n} \sum_{i=1}^{n} (b)  = \frac{0}{2} \]
\[ -\frac{1}{n} \sum_{i=1}^{n} y_i +\frac{m}{n} \sum_{i=1}^{n} {x_i}  + b  = 0 \]
\[ -\overline{y} + m \times \overline{x}  + b  = 0 \hspace{10mm} \ldots (1) \]
</p>

<p>Equating the partial derivatives of the MSE with respect to m to zero:</p>

<p>
\[ \frac{\partial \text{MSE}}{\partial m} = -\frac{2}{n} \sum_{i=1}^{n} x_i (y_i - (m {x_i} + b)) = 0 \]
\[ -\frac{2}{n} \sum_{i=1}^{n} (x_i \times y_i - x_i \times  (m {x_i} + b)) = 0 \]
\[ -\frac{2}{n} \sum_{i=1}^{n} (x_i  y_i - m {x_i}^2 - x_i b) = 0 \]
\[ -\frac{1}{n} \sum_{i=1}^{n} (x_i  y_i - m {x_i}^2 - x_i b) = \frac{0}{2} \]
\[ -\frac{1}{n} \sum_{i=1}^{n} (x_i  y_i) +\frac{1}{n} \sum_{i=1}^{n} (m {x_i}^2) +\frac{1}{n} \sum_{i=1}^{n} (x_i b) = 0 \]
\[ -\overline{xy} + m \times \overline{x^2} + b \times \overline{x} = 0 \hspace{10mm} \ldots (2) \]
</p>

<p>Now we have two equations and two equations so now solving them together</p>

<p>
using equation one we have
\[ -\overline{y} + m \times \overline{x}  + b  = 0 \]
\[ b = \overline{y} - m \times \overline{x} \]

puting this value of b in equation two we have
\[ -\overline{xy} + m \times \overline{x^2} + b \times \overline{x} = 0 \]
\[ -\overline{xy} + m \times \overline{x^2} + (\overline{y} - m \times \overline{x}) \times \overline{x} = 0 \]
\[ -\overline{xy} + m \times \overline{x^2} + \overline{y} \times \overline{x} - m{\overline{x}}^2 = 0 \]
\[ -\overline{xy} + \overline{y} \times \overline{x} + m (\overline{x^2} - {\overline{x}}^2) = 0 \]
\[ m (\overline{x^2} - {\overline{x}}^2) = \overline{xy} - \overline{y} \times \overline{x}  \]
\[ m (\frac{1}{n} \sum_{i=1}^{n} x_i^2 - (\frac{1}{n} \sum_{i=1}^{n} x_i)^2) = \frac{1}{n} \sum_{i=1}^{n} x_i  y_i - \frac{1}{n} \sum_{i=1}^{n} y_i \times \frac{1}{n} \sum_{i=1}^{n} x_i  \]
\[ mn^2 (\frac{1}{n} \sum_{i=1}^{n} x_i^2 - (\frac{1}{n} \sum_{i=1}^{n} x_i)^2) = n \sum_{i=1}^{n} x_i  y_i - \sum_{i=1}^{n} y_i \times \sum_{i=1}^{n} x_i  \]
\[ m (n \sum_{i=1}^{n} x_i^2 - n^2(\frac{1}{n} \sum_{i=1}^{n} x_i)^2) = n \sum_{i=1}^{n} x_i  y_i - \sum_{i=1}^{n} y_i \sum_{i=1}^{n} x_i  \]
\[ m (n \sum_{i=1}^{n} x_i^2 - (\sum_{i=1}^{n} x_i)^2) = n \sum_{i=1}^{n} x_i  y_i - \sum_{i=1}^{n} y_i \sum_{i=1}^{n} x_i  \]
\[ m (n \sum_{}^{} x^2 - (\sum_{}^{} x)^2) = n \sum_{}^{} x y - \sum_{}^{} y \sum_{}^{} x \]
\[ m = \frac{n \sum_{}^{} x y - \sum_{}^{} x \sum_{}^{} y}{n \sum_{}^{} x^2 - (\sum_{}^{} x)^2} \]
</p>

<p>Now using equation two we have</p>

<p>
\[ -\overline{xy} + m \times \overline{x^2} + b \times \overline{x} = 0 \]
\[ m \times \overline{x^2} = \overline{xy} - b \times \overline{x} \]
\[ m = (\overline{xy} - b \times \overline{x})/(\overline{x^2}) \]
</p>

<p>on puting this value of m in equation one we have</p>

<p>
\[ -\overline{y} + m \times \overline{x} + b  = 0 \]
\[ -\overline{y} + (\overline{xy} - b \times \overline{x})/(\overline{x^2}) \times \overline{x} + b  = 0 \]
\[ -\overline{y} \times \overline{x^2} + (\overline{xy} - b \times \overline{x}) \times \overline{x} + b \times \overline{x^2}  = 0 \]
\[ -\overline{y} \times \overline{x^2} + \overline{xy} \times \overline{x} - b \times \overline{x} \times \overline{x} + b \times \overline{x^2}  = 0 \]
\[ -\overline{y} \times \overline{x^2} + \overline{xy} \times \overline{x} + b (\overline{x^2} - \overline{x} \times \overline{x})  = 0 \]
\[ b (\overline{x^2} - \overline{x} \times \overline{x}) = \overline{y} \times \overline{x^2} - \overline{xy} \times \overline{x}  \]
\[ b (\overline{x^2} - {\overline{x}}^2) = \overline{y} \times \overline{x^2} - \overline{xy} \times \overline{x}  \]
\[ b (\frac{1}{n} \sum_{}^{} x^2 - (\frac{1}{n} \sum_{}^{} x)^2) = \frac{1}{n} \sum_{}^{} y \times \frac{1}{n} \sum_{}^{} x^2 - \frac{1}{n} \sum_{}^{} xy \times \frac{1}{n} \sum_{}^{} x \]
\[ bn^2 (\frac{1}{n} \sum_{}^{} x^2 - (\frac{1}{n} \sum_{}^{} x)^2) = \sum_{}^{} y \times \sum_{}^{} x^2 - \sum_{}^{} xy \times \sum_{}^{} x \]
\[ b(n \sum_{}^{} x^2 - n^2(\frac{1}{n} \sum_{}^{} x)^2) = \sum_{}^{} y \sum_{}^{} x^2 - \sum_{}^{} xy \sum_{}^{} x \]
\[ b(n \sum_{}^{} x^2 - (\sum_{}^{} x)^2) = \sum_{}^{} y \sum_{}^{} x^2 - \sum_{}^{} xy \sum_{}^{} x \]
\[ b = \frac{\sum_{}^{} y \sum_{}^{} x^2 - \sum_{}^{} xy \sum_{}^{} x}{n \sum_{}^{} x^2 - (\sum_{}^{} x)^2} \]
</p>

<h3>For line y = ax + b</h3>

<p><img src="https://cdn1.byjus.com/wp-content/uploads/2019/11/linear-regression-formula.png" alt="https://cdn1.byjus.com/wp-content/uploads/2019/11/linear-regression-formula.png" /> <figcaption>https://cdn1.byjus.com/wp-content/uploads/2019/11/linear-regression-formula.png</figcaption></p>

<p>We just derive the linear regression formula which you use in your physics labs. But this is just for one dimensional feature vector. But don't worry we can easily modify this for linear regression of n dimensional feature vector.</p>

<div class="separator"><span>‚Ä¢ ‚Ä¢ ‚Ä¢</span></div>

<h2>Section 3: Python Implementation Up to This point</h2>

<pre><code class="python">import numpy as np
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

X, Y = make_regression(n_samples=10000, n_features=1, noise=50)
X = X.reshape(-1)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)
def fit(X, Y):
    assert len(X.shape) == len(Y.shape) and len(X.shape) == 1, &quot;X and Y must one dimensional i.e., (n, )&quot;
    n=len(X)   
    sum_y = Y.sum()
    sum_x = X.sum()
    sum_xy = (X*Y).sum()
    sum_xx = (X*X).sum()
    m = (n*sum_xy - sum_x*sum_y)/(n*sum_xx-sum_x*sum_x)
    b = (sum_y*sum_xx - sum_xy*sum_x)/(n*sum_xx-sum_x*sum_x)
    return lambda x: m*x+b
model = fit(X_train, Y_train)

predictions = model(X_test)
print(f&quot;MSE: {mean_squared_error(predictions, Y_test):.5f}&quot;)
print(f&quot;R^2 Score: {r2_score(predictions, Y_test):.5f}&quot;)

from sklearn import linear_model
sklearn_model = linear_model.LinearRegression()
X_train = X_train.reshape(-1, 1)
X_test = X_test.reshape(-1, 1)
sklearn_model.fit(X_train, Y_train)
sklearn_predictions = sklearn_model.predict(X_test)
print(f&quot;sklearn MSE: {mean_squared_error(sklearn_predictions, Y_test):.5f}&quot;)
print(f&quot;sklearn R^2 Score: {r2_score(sklearn_predictions, Y_test):.5f}&quot;)</code></pre>

<p>Output:</p>

<pre><code class="bash">MSE: 2333.91338
R^2 Score: -3.07210
sklearn MSE: 2333.91338
sklearn R^2 Score: -3.07210</code></pre>

<h2>Section 4: Linear Regression for multiple variables</h2>

<p>Now we have new linear regression formula i.e., </p>

<p>
\[ \hat{y} = {M}{X} + b \]
or
\[ \hat{y} = m_1 x_1 + m_2 x_2 + \ldots + m_j x_j + b \]
\[ \hat{y} = b + \sum_{i=1}^{n} {m_j} {x_j} \]
</p>

<h3>Again Doing Partial Derivatives</h3>

<p>The partial derivatives of the MSE with respect to b:</p>

<p>
\[ \frac{\partial \text{MSE}}{\partial b} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2 \]
\[ \frac{\partial \text{MSE}}{\partial b} = \frac{1}{n} \sum_{i=1}^{n} (y_i - (b + \sum_{}^{} {m_j} {x_ij}))^2 \]
\[ \frac{\partial \text{MSE}}{\partial b} = \frac{1}{n} \sum_{i=1}^{n} (y_i - (b + \sum_{}^{} {m_j} {x_ij}))(-2) \]
\[ \frac{\partial \text{MSE}}{\partial b} = -\frac{2}{n} \sum_{i=1}^{n} (y_i - (b + \sum_{}^{} {m_j} {x_ij})) \]
\[ \frac{\partial \text{MSE}}{\partial b} = -\frac{2}{n} \sum_{i=1}^{n} (y_i - \hat{y_i}) \]
</p>

<p>The partial derivatives of the MSE with respect to mj:</p>

<p>
\[ \frac{\partial \text{MSE}}{\partial m_j} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2 \]
\[ \frac{\partial \text{MSE}}{\partial m_j} = \frac{1}{n} \sum_{i=1}^{n} (y_i - (b + \sum_{}^{} {m_j} {x_ij}))^2 \]
\[ \frac{\partial \text{MSE}}{\partial m_j} = \frac{1}{n} \sum_{i=1}^{n} (y_i - (b + \sum_{}^{} {m_j} {x_ij}))(-2{x_ij}) \]
\[ \frac{\partial \text{MSE}}{\partial m_j} = -\frac{2}{n} \sum_{i=1}^{n} x_ij (y_i - (b + \sum_{}^{} {m_j} {x_ij})) \]
\[ \frac{\partial \text{MSE}}{\partial m_j} = -\frac{2}{n} \sum_{i=1}^{n} x_ij (y_i - \hat{y_i}) \]
</p>

<h3>Again Equating the gradients of the error function to zero</h3>

<p>Lets assume that we have k dimensional feature vector.</p>

<p>Equating the partial derivatives of the MSE with respect to b to zero:</p>

<p>
\[ \frac{\partial \text{MSE}}{\partial b} = -\frac{2}{n} \sum_{i=1}^{n} (y_i - (b + \sum_{}^{} {m_j} {x_ij})) = 0 \]
\[ -\frac{1}{n} \sum_{i=1}^{n} (y_i - b - \sum_{}^{} {m_j} {x_ij}) = \frac{0}{2} \]
\[ -\frac{1}{n} \sum_{i=1}^{n} (y_i - b - \sum_{}^{} {m_j} {x_ij}) = 0 \]
\[ -\frac{1}{n} \sum_{i=1}^{n} y_i + \frac{1}{n} \sum_{i=1}^{n} b + \frac{1}{n} \sum_{i=1}^{n} (\sum_{}^{} {m_j} {x_ij}) = 0 \]
\[ -\overline{y} + b + \frac{1}{n} \sum_{i=1}^{n} (\sum_{}^{} {m_j} {x_ij}) = 0 \]
\[ -\overline{y} + b + \frac{1}{n} \sum_{i=1}^{n} (m_1 x_i1 + m_2 x_i2 + \ldots + m_j x_ij) = 0 \]
\[ -\overline{y} + b + (\frac{1}{n} \sum_{i=1}^{n} m_1 x_i1 + \frac{1}{n} \sum_{i=1}^{n} m_2 x_i2 + \ldots + \frac{1}{n} \sum_{i=1}^{n} m_j x_ij) = 0 \]
\[ -\overline{y} + b + (m_1 \overline{x_i1} + m_2 \overline{x_i2} + \ldots + m_j \overline{x_ij}) = 0 \]
\[ -\overline{y} + b + \sum_{j=1}^{k}(m_j \overline{x_ij}) = 0 \hspace{10mm} \ldots (3) \]
</p>

<p>Equating the partial derivatives of the MSE with respect to mj' to zero:</p>

<p>
\[ \frac{\partial \text{MSE}}{\partial m_j*} = -\frac{2}{n} \sum_{i=1}^{n} x_ij' (y_i - (b + \sum_{}^{} {m_j} {x_ij})) = 0 \]
\[ -\frac{1}{n} \sum_{i=1}^{n} x_ij' (y_i - b - \sum_{}^{} {m_j} {x_ij}) = \frac{0}{2} \]
\[ -\frac{1}{n} \sum_{i=1}^{n} x_ij' (y_i - b - \sum_{}^{} {m_j} {x_ij}) = 0 \]
\[ -\frac{1}{n} \sum_{i=1}^{n} x_ij' y_i + \frac{1}{n} \sum_{i=1}^{n} x_ij' b + \frac{1}{n} \sum_{i=1}^{n} (x_ij' \sum_{}^{} {m_j} {x_ij}) = 0 \]
\[ -\overline{x_ij' y_i} +  b \overline{x_ij'} + \frac{1}{n} \sum_{i=1}^{n} (x_ij' \sum_{}^{} {m_j} {x_ij}) = 0 \]
\[ -\overline{x_ij' y_i} +  b \overline{x_ij'} + \frac{1}{n} \sum_{i=1}^{n} (x_ij' \times (m_1 x_i1 + m_2 x_i2 + \ldots + m_j x_ij)) = 0 \]
\[ -\overline{x_ij' y_i} +  b \overline{x_ij'} + \frac{1}{n} \sum_{i=1}^{n} (x_ij' \times m_1 x_i1 + x_ij' \times m_2 x_i2 + \ldots + x_ij' \times m_j x_ij) = 0 \]
\[ -\overline{x_ij' y_i} +  b \overline{x_ij'} + (\frac{1}{n} \sum_{i=1}^{n} x_ij' m_1 x_i1 + \frac{1}{n} \sum_{i=1}^{n} x_ij' m_2 x_i2 + \ldots + \frac{1}{n} \sum_{i=1}^{n} x_ij' m_j x_ij) = 0 \]
\[ -\overline{x_ij' y_i} +  b \overline{x_ij'} + (m_1 \overline{x_ij' x_i1} + m_2 \overline{x_ij' x_i2} + \ldots + m_j \overline{x_ij' x_ij}) = 0 \]
\[ -\overline{x_ij' y_i} +  b \overline{x_ij'} + \sum_{j=1}^{k} (m_j \overline{x_ij' x_ij}) = 0 \hspace{10mm} \ldots (4) \]
</p>

<p>using equation 3 we have</p>

<p>
\[ -\overline{y} + b + \sum_{j=1}^{k}(m_j \overline{x_ij}) = 0 \]
\[ b = \overline{y} - \sum_{j=1}^{k}(m_j \overline{x_ij}) \]
on puting this value of b in equation 4 we have
\[ -\overline{x_ij' y_i} +  b \overline{x_ij'} + \sum_{j=1}^{k} (m_j \overline{x_ij' x_ij}) = 0\]
\[ -\overline{x_ij' y_i} + (\overline{y} - \sum_{j=1}^{k}(m_j \overline{x_ij})) \times \overline{x_ij'} + \sum_{j=1}^{k} (m_j \overline{x_ij' x_ij}) = 0\]
\[ -\overline{x_ij' y_i} + \overline{y} \times \overline{x_ij'} - \overline{x_ij'} \times \sum_{j=1}^{k}(m_j \overline{x_ij}) + \sum_{j=1}^{k} (m_j \overline{x_ij' x_ij}) = 0\]
\[ -\overline{x_ij' y_i} + \overline{y} \times \overline{x_ij'} - \sum_{j=1}^{k}(\overline{x_ij'} m_j \overline{x_ij}) + \sum_{j=1}^{k} (m_j \overline{x_ij' x_ij}) = 0\]
\[ -\overline{x_ij' y_i} + \overline{y} \times \overline{x_ij'} - \sum_{j=1}^{k}(\overline{x_ij'} m_j \overline{x_ij} - m_j \overline{x_ij' x_ij}) = 0\]
\[ -\overline{x_ij' y_i} + \overline{y} \times \overline{x_ij'} - \sum_{j=1}^{k} m_j (\overline{x_ij'} \times \overline{x_ij} - \overline{x_ij' x_ij}) = 0\]
\[ \sum_{j=1}^{k} m_j (\overline{x_ij'} \times \overline{x_ij} - \overline{x_ij' x_ij}) = \overline{y} \times \overline{x_ij'} - \overline{x_ij' y_i} \]
\[ \sum_{j=1}^{k} m_j (\overline{x_ij'} \times \overline{x_ij}) - \sum_{j=1}^{k} m_j (\overline{x_ij' x_ij}) = \overline{y} \times \overline{x_ij'} - \overline{x_ij' y_i} \]

Now we have k such equation
\[ \sum_{j=1}^{k} m_j (\overline{x_i1} \times \overline{x_ij}) - \sum_{j=1}^{k} m_j (\overline{x_i1 x_ij}) = \overline{y} \times \overline{x_i1} - \overline{x_i1 y_i} \]
\[ \sum_{j=1}^{k} m_j (\overline{x_i2} \times \overline{x_ij}) - \sum_{j=1}^{k} m_j (\overline{x_i2 x_ij}) = \overline{y} \times \overline{x_i2} - \overline{x_i2 y_i} \]
\[ \ldots \]
\[ \ldots \]
\[ \ldots \]
\[ \sum_{j=1}^{k} m_j (\overline{x_ij'} \times \overline{x_ij}) - \sum_{j=1}^{k} m_j (\overline{x_ij' x_ij}) = \overline{y} \times \overline{x_ij'} - \overline{x_ij' y_i} \]
</p>

<p>Now, We have k equations and k unknown variables so we can find the unknown variables using equation 3 and equations 4.</p>

<h3>Python Implementation</h3>

<pre><code class="python">import numpy as np
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

X, Y = make_regression(n_samples=10000, n_features=5, noise=50)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)
def fit(X:np.ndarray, Y:np.ndarray):
    n=len(X)
    avg_y = Y.mean()
    avg_xj = X.mean(axis=0)
    avg_yxj = np.mean(Y.reshape(-1, 1) * X, axis=0) # np.mean(Y[:, np.newaxis] * X, axis=0)

    avg_xjxj = np.dot(X.T, X) / n

    B = avg_y * avg_xj - avg_yxj # shape = (k)
    A = np.outer(avg_xj, avg_xj) - avg_xjxj # shape = (k, k)
    M = np.linalg.solve(A, B)
    b = avg_y - np.dot(M, avg_xj) # avg_y - (avg_xj * M).sum() 
    return M, b

M, b = fit(X_train, Y_train)
model = lambda X: np.dot(X, M) + b
predictions = model(X_test)
print(f&quot;MSE: {mean_squared_error(predictions, Y_test):.5f}&quot;)
print(f&quot;R^2 Score: {r2_score(predictions, Y_test):.5f}&quot;)

from sklearn import linear_model
sklearn_model = linear_model.LinearRegression()
sklearn_model.fit(X_train, Y_train)
sklearn_predictions = sklearn_model.predict(X_test)
print(f&quot;sklearn MSE: {mean_squared_error(sklearn_predictions, Y_test):.5f}&quot;)
print(f&quot;sklearn R^2 Score: {r2_score(sklearn_predictions, Y_test):.5f}&quot;)</code></pre>

<p>Output:</p>

<pre><code class="bash">MSE: 2649.37218
R^2 Score: 0.79578
sklearn MSE: 2649.37218
sklearn R^2 Score: 0.79578</code></pre>

<h3>How I efficiently calculate A</h3>

<p>
\[ \sum_{j=1}^{k} m_j (\overline{x_i1} \times \overline{x_ij}) - \sum_{j=1}^{k} m_j (\overline{x_i1 x_ij}) = \overline{y} \times \overline{x_i1} - \overline{x_i1 y_i} \]
\[ \sum_{j=1}^{k} m_j (\overline{x_i2} \times \overline{x_ij}) - \sum_{j=1}^{k} m_j (\overline{x_i2 x_ij}) = \overline{y} \times \overline{x_i2} - \overline{x_i2 y_i} \]
\[ \ldots \]
\[ \ldots \]
\[ \ldots \]
\[ \sum_{j=1}^{k} m_j (\overline{x_ij'} \times \overline{x_ij}) - \sum_{j=1}^{k} m_j (\overline{x_ij' x_ij}) = \overline{y} \times \overline{x_ij'} - \overline{x_ij' y_i} \]
</p>

<h3>For A (left side equation)</h3>

<p>I calculate the first summation using <code>np.outer(avg_xj, avg_xj)</code></p>

<p>Then calculate the second summation using <code>avg_xjxj = np.dot(X.T, X) / n</code></p>

<p>Heres how i calculate it using an example where X have (n=2, k=3) shape</p>

<p>We Wants to calculate the following matrix</p>

<p>
$$\begin{pmatrix}(x_11 x_11 + x_21 x_21) & (x_11 x_12 + x_21 x_22) & (x_11 x_13 + x_21 x_23)\\\ (x_12 x_11 + x_22 x_21) & (x_12 x_12 + x_22 x_22) & (x_12 x_13 + x_22 x_23)\\\ (x_13 x_11 + x_23 x_21) & (x_13 x_12 + x_23 x_22) & (x_13 x_13 + x_23 x_23)\end{pmatrix}$$
Or
$$\begin{pmatrix}x_11 & x_21 \\\ x_12 & x_22 \\\ x_13 & x_23 \end{pmatrix} \times \begin{pmatrix}x_11 & x_12 & x_13 \\\ x_21 & x_22 & x_23 \end{pmatrix}$$
</p>

<p>Then i subtract it from the first summation.</p>

<h3>For B (right side equation)</h3>

<p>I simply use the same expression <code>avg_y * avg_xj - avg_yxj</code></p>

<h3>Now i solve the System of linear equations</h3>

<p>Now i solve the system of linear equations using <code>np.linalg.solve(A, B)</code> Which gives the slope of the linear regression.</p>

<h3>find the intercept</h3>

<p>To find the b i just use the same expression <code>avg_y - (avg_xj * M).sum()</code> but in optimized way <code>avg_y - np.dot(M, avg_xj)</code>.</p>

<p>
\[ b = \overline{y} - \sum_{j=1}^{k}(m_j \overline{x_ij}) \]
</p>

<h2>Full Code</h2>

<p>you can <a href="https://github.com/thefcraft/thefcraft-ai-ml/blob/main/linearRegression.py">download full code</a> from my <a href="https://github.com/thefcraft">github account</a>.</p>
<div class="tags" id="tag-footer">
                <a href="/search/index.html?tag=Machine_Learning">Machine Learning</a><a href="/search/index.html?tag=Linear_Regression">Linear Regression</a><a href="/search/index.html?tag=Mathematics">Mathematics</a><a href="/search/index.html?tag=Python">Python</a>
            </div><div class="blog-reaction-div" id="reaction-footer" style="border:none">
                <div class="blog-reaction">
                    <div>
                        <svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" fill="currentColor" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z"></path></svg>
                    </div>
                    <div>141</div>
                    <div style="margin-left: 16px;">
                        <svg width="24" height="24" viewBox="0 0 24 24" class="jp"><path fill="currentColor" d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg>
                    </div>
                    <div>849</div>
                </div>
                <div class="blog-save-share">
                    <div id="share-footer">
                        <svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z" fill="currentColor"></path></svg>
                    </div>
                </div>
            </div>
            
         
        </div>
    </div>
</div>

<div class="root footer-bg">
    <div class="container">
        <div class="writer">
            <div class="writer-img">
                <img alt="ThefCraft" src="/static/userPNG/c5f67cbc-b58f-46cc-864a-5e48b2a6d582.jpg" width="72" height="72" loading="lazy">
            </div>
            <div style="display: flex;flex-wrap: wrap; flex-direction: column;">
                <div style="padding-bottom: 1rem;">
                    <h2>Written by ThefCraft</h2>
                    <div style="margin-bottom:8px;">
                        <a href="">00 Followers</a>
                    </div>
                    <p>
                        Explore the dynamic fusion of mathematics and computing through the eyes of ThefCraft, a B.Tech. student at IIT Patna. Delve into the depths of algorithms, data analysis, artificial intelligence, deep learning and computational techniques in this captivating blog. Join ThefCraft on a journey of innovation and discovery at the intersection of two powerful disciplines.
                    </p>
                </div>
                <div class="newsletter" id="newsletter">
                    <h2>Sign up for my newsletter</h2>
                    <p>Get notified when I post a new article. Unsubscribe at any time, and I promise not to send any spam :)</p>
                    <form action="https://formspree.io/f/mbjnelqe" method="POST">
                    <input type="email" id="email" name="EMAIL" autocomplete="email" required="" placeholder="Your email">
                    <button style="padding: 0px;" type="submit">
                        <div style="display:flex; align-items: center; padding: 4px 8px;">
                        Follow
                        <svg width="38" height="38" viewBox="0 0 38 38" fill="none" style="stroke: rgb(255, 255, 255);width: 36px;height: 36px;"><rect x="26.25" y="9.25" width="0.5" height="6.5" rx="0.25"></rect><rect x="29.75" y="12.25" width="0.5" height="6.5" rx="0.25" transform="rotate(90 29.75 12.25)"></rect><path d="M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5"></path><path d="M11.5 14.5L19 20l4-3"></path></svg>
                        </div>
                    </button>
                    </form>
                </div>
            </div>
        </div>
        
        
    </div>
</div>

<footer>
    <a href="/about">About</a>
    <a href="https://github.com/thefcraft/blog">Blog</a>
    <a href="/new">Write</a>
    <a href="/privacy">Privacy</a>
    <a href="/terms">Terms</a>
    <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API">Text to speech</a>
    <a href="https://github.com/thefcraft/blog/graphs/contributors">Teams</a>
</footer>

<script type="text/javascript" src="/static/js/blog.js"></script>

    <script>

        
        const checkbox = document.getElementById("checkbox")
        checkbox.checked = !isDarkMode();
        checkbox.addEventListener("change", () => {
            localStorage.setItem('darkmode', !checkbox.checked);
            console.log("darkmode set to: ", !checkbox.checked);
            if (checkbox.checked) removeDarkMode();
            else addDarkMode();
        })

        let darkmode = localStorage.getItem('darkmode');
        if (darkmode != null) {
            console.log(darkmode);
            if (darkmode==='true') {
                addDarkMode();
                checkbox.checked = false;
            }
        }else if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
                addDarkMode();
                checkbox.checked = false;
        }
    </script>

</body>
</html>